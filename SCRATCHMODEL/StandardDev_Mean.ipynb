{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC - ROC on Test data average prediction scores: 0.7250000000000001\n",
      "AUC - ROC on Test data maximum prediction scores: 0.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve,roc_auc_score,precision_recall_curve,auc\n",
    "list_avg = []\n",
    "\n",
    "\n",
    "\n",
    "df_1 = pd.read_csv(\"Desktop/Dataset/model_scratch/smallmodel_test_fold1_trail3.csv\")\n",
    "average_fold1 = df_1['Average'].tolist()\n",
    "actual_labelsfold1 = df_1['Actual labels'].tolist()\n",
    "maximum_fold1 = df_1['Maximum'].tolist()\n",
    "\n",
    "false_pr_avg,true_pr_avg,threshold = roc_curve(actual_labelsfold1,average_fold1)\n",
    "roc_auc_avg1 = auc(false_pr_avg,true_pr_avg)\n",
    "print(\"AUC - ROC on Test data average prediction scores:\",roc_auc_avg1)\n",
    "list_avg.append(roc_auc_avg1)\n",
    "\n",
    "\n",
    "false_pr_avg,true_pr_avg,threshold = roc_curve(actual_labelsfold1,maximum_fold1)\n",
    "roc_auc_avg = auc(false_pr_avg,true_pr_avg)\n",
    "print(\"AUC - ROC on Test data maximum prediction scores:\",roc_auc_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC - ROC on validation data average prediction scores: 0.6513157894736842\n",
      "AUC - ROC on validation data maximum prediction scores: 0.5592105263157895\n"
     ]
    }
   ],
   "source": [
    "df_1 = pd.read_csv(\"Desktop/Dataset/model_scratch/smallmodel_val_fold1_trail3.csv\")\n",
    "average_fold1 = df_1['Average'].tolist()\n",
    "actual_labelsfold1 = df_1['Actual labels'].tolist()\n",
    "maximum_fold1 = df_1['Maximum'].tolist()\n",
    "\n",
    "false_pr_avg,true_pr_avg,threshold = roc_curve(actual_labelsfold1,average_fold1)\n",
    "roc_auc_avg = auc(false_pr_avg,true_pr_avg)\n",
    "print(\"AUC - ROC on validation data average prediction scores:\",roc_auc_avg)\n",
    "\n",
    "false_pr_avg,true_pr_avg,threshold = roc_curve(actual_labelsfold1,maximum_fold1)\n",
    "roc_auc_avg = auc(false_pr_avg,true_pr_avg)\n",
    "print(\"AUC - ROC on validation data maximum prediction scores:\",roc_auc_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>FOLD2</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC - ROC on Test data average prediction scores: 0.6499999999999999\n",
      "AUC - ROC on Test data maximum prediction scores: 0.5\n"
     ]
    }
   ],
   "source": [
    "df_2 = pd.read_csv(\"Desktop/Dataset/model_scratch/smallmodel_test_fold2_trail3.csv\")\n",
    "average_fold2 = df_2['Average'].tolist()\n",
    "actual_labelsfold2 = df_2['Actual labels'].tolist()\n",
    "maximum_fold2 = df_2['Maximum'].tolist()\n",
    "\n",
    "false_pr_avg,true_pr_avg,threshold = roc_curve(actual_labelsfold2,average_fold2)\n",
    "roc_auc_avg2 = auc(false_pr_avg,true_pr_avg)\n",
    "print(\"AUC - ROC on Test data average prediction scores:\",roc_auc_avg2)\n",
    "list_avg.append(roc_auc_avg2)\n",
    "\n",
    "false_pr_avg,true_pr_avg,threshold = roc_curve(actual_labelsfold2,maximum_fold2)\n",
    "roc_auc_avg = auc(false_pr_avg,true_pr_avg)\n",
    "print(\"AUC - ROC on Test data maximum prediction scores:\",roc_auc_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC - ROC on validation data average prediction scores: 0.7374999999999999\n",
      "AUC - ROC on validation data maximum prediction scores: 0.5\n"
     ]
    }
   ],
   "source": [
    "df_2_val = pd.read_csv(\"Desktop/Dataset/model_scratch/smallmodel_val_fold2_trail3.csv\")\n",
    "average_fold2_val = df_2_val['Average'].tolist()\n",
    "actual_labelsfold2_val = df_2_val['Actual labels'].tolist()\n",
    "maximum_fold2_val = df_2_val['Maximum'].tolist()\n",
    "\n",
    "false_pr_avg,true_pr_avg,threshold = roc_curve(actual_labelsfold2_val,average_fold2_val)\n",
    "roc_auc_avg = auc(false_pr_avg,true_pr_avg)\n",
    "print(\"AUC - ROC on validation data average prediction scores:\",roc_auc_avg)\n",
    "\n",
    "false_pr_avg,true_pr_avg,threshold = roc_curve(actual_labelsfold2_val,maximum_fold2_val)\n",
    "roc_auc_avg = auc(false_pr_avg,true_pr_avg)\n",
    "print(\"AUC - ROC on validation data maximum prediction scores:\",roc_auc_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>FOLD3</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC - ROC on Test data average prediction scores: 0.675\n",
      "AUC - ROC on Test data maximum prediction scores: 0.5\n"
     ]
    }
   ],
   "source": [
    "df_3_test = pd.read_csv(\"Desktop/Dataset/model_scratch/smallmodel_test_fold3_trail3.csv\")\n",
    "average_fold3 = df_3_test['Average'].tolist()\n",
    "actual_labelsfold3 = df_3_test['Actual labels'].tolist()\n",
    "maximum_fold3 = df_3_test['Maximum'].tolist()\n",
    "\n",
    "false_pr_avg,true_pr_avg,threshold = roc_curve(actual_labelsfold3,average_fold3)\n",
    "roc_auc_avg3 = auc(false_pr_avg,true_pr_avg)\n",
    "print(\"AUC - ROC on Test data average prediction scores:\",roc_auc_avg3)\n",
    "list_avg.append(roc_auc_avg3)\n",
    "\n",
    "false_pr_avg,true_pr_avg,threshold = roc_curve(actual_labelsfold3,maximum_fold3)\n",
    "roc_auc_avg = auc(false_pr_avg,true_pr_avg)\n",
    "print(\"AUC - ROC on Test data maximum prediction scores:\",roc_auc_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC - ROC on validation data average prediction scores: 0.6875\n",
      "AUC - ROC on validation data maximum prediction scores: 0.5\n"
     ]
    }
   ],
   "source": [
    "df_3_val = pd.read_csv(\"Desktop/Dataset/model_scratch/smallmodel_val_fold3_trail3.csv\")\n",
    "average_fold3_val = df_3_val['Average'].tolist()\n",
    "actual_labelsfold3_val = df_3_val['Actual labels'].tolist()\n",
    "maximum_fold3_val = df_3_val['Maximum'].tolist()\n",
    "\n",
    "false_pr_avg,true_pr_avg,threshold = roc_curve(actual_labelsfold3_val,average_fold3_val)\n",
    "roc_auc_avg = auc(false_pr_avg,true_pr_avg)\n",
    "print(\"AUC - ROC on validation data average prediction scores:\",roc_auc_avg)\n",
    "\n",
    "false_pr_avg,true_pr_avg,threshold = roc_curve(actual_labelsfold3_val,maximum_fold3_val)\n",
    "roc_auc_avg = auc(false_pr_avg,true_pr_avg)\n",
    "print(\"AUC - ROC on validation data maximum prediction scores:\",roc_auc_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VARIANCE AND MEAN ACROSS THREE DIFFERENT FOLDS ON TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Across three folds : 0.6833333333333333\n"
     ]
    }
   ],
   "source": [
    "import statistics as st\n",
    "\n",
    "print(\"Average Across three folds :\",st.mean(list_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD of three folds : 0.03818813079129875\n"
     ]
    }
   ],
   "source": [
    "import statistics as st\n",
    "print(\"STD of three folds :\",st.stdev(list_avg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
